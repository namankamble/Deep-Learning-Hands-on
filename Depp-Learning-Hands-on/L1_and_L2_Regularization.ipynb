{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d738c87a-dd3b-49e1-890b-3801461d1d3d",
   "metadata": {},
   "source": [
    "***\n",
    "# <center>***L1 and L2 Regularization***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbc323-a251-4156-99ee-8e90380d2157",
   "metadata": {},
   "source": [
    "**Regularization methods** are those which **reduce generalization error**. The first forms of regularization that we will address are **L1** and **L2** **regularization** to calculate a number (called a **penalty**. L1 and L2 regularization are used) added to the loss value to penalize the model for large weights and biases. Large weights might indicate that a neuron is attempting to memorize a data element; generally, it is believed that it would be better to have many neurons contributing to a model’s output, rather than a select few. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c96aed-df99-4e3b-9130-e2fa61232685",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Forward Pass***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463b12e-fd99-4b38-b499-727a0ab8867a",
   "metadata": {},
   "source": [
    "**L1 regularization’s** penalty is the sum of all the absolute values for the weights and biases. This is a linear penalty as regularization loss returned by this function is directly proportional to parameter values. **L2 regularization’s** penalty is the sum of the squared weights and biases. This non-linear approach penalizes larger weights and biases more than smaller ones because of the square function used to calculate the result. In other words, **L2 regularization** is commonly used as it does not affect small parameter values substantially and does not allow the model to grow weights too large by heavily penalizing relatively big values. L1 regularization, because of its linear nature, penalizes small weights more than L2 regularization, causing the model to start being invariant to small inputs and variant only to the bigger ones. That’s why L1 regularization is rarely used alone and usually combined with L2 regularization if it’s even used at all. Regularization functions of this type drive the sum of weights and the sum of parameters towards 0​, which can also help in cases of exploding gradients (model instability, which might cause weights to become very large values). Beyond this, we also want to dictate how much of an impact we want this regularization penalty to carry. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a71649-7477-4ec0-b574-ee2c509609ef",
   "metadata": {},
   "source": [
    "### **L1 Regularization**\n",
    " **L1 Weight Regularization:**\n",
    "$$L_{1w} = \\lambda \\sum_{m} |w_{m}| $$\n",
    "*Explanation:*\n",
    " - This term is added to the loss function during training.\n",
    " - It penalizes the model for having large weights.\n",
    " - The absolute value (|w_m|) encourages the weights to become sparse, meaning many weights will become exactly zero.\n",
    " - This can help in feature selection, as it effectively removes the influence of irrelevant features from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9183b7-4ca1-47ea-bf0c-03eb8c36ea2e",
   "metadata": {},
   "source": [
    "**L1 Bias Regularization:**\n",
    "$$L_{1b} = \\lambda \\sum_{n} |b_{n}|$$\n",
    "*Explanation:*\n",
    " - Similar to L1 weight regularization, this term penalizes the model for having large biases.\n",
    " - It encourages the biases to also become sparse, which can further simplify the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd3a28-7424-4323-bafc-8336c3f3fadd",
   "metadata": {},
   "source": [
    "### **L2 Regularization**\n",
    "\n",
    "**L1 Weight Regularization:**\n",
    "$$L_{2w} = \\lambda \\sum_{m} w_{m}^2$$\n",
    "*Explanation:*\n",
    " - This term is also added to the loss function.\n",
    " - It penalizes the model for having large weights, but in a different way than L1.\n",
    " - It encourages the weights to be small, but not necessarily zero.\n",
    " - L2 regularization is often called weight decay because it tends to shrink the weights during training.\n",
    "\n",
    "**L2 Bias Regularization:**\n",
    "$$L_{2b} = \\lambda \\sum_{n} b_{n}^2$$\n",
    "*Explanation:*\n",
    "\n",
    " - This term is added to the loss function during training.\n",
    " - It penalizes the model for having large biases.\n",
    " - It encourages the biases to be small, but not necessarily zero.\n",
    " - Similar to L2 weight regularization, it tends to shrink the biases during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a472d94-7d05-4417-9e62-181a30d06e15",
   "metadata": {},
   "source": [
    "**Overall Loss:**\n",
    "$$Loss = DataLoss + L_{1w} + L_{1b} + L_{2w} + L_{2b}$$\n",
    "*Explanation:*\n",
    "\n",
    " - This equation represents the total loss that the model tries to minimize during training.\n",
    " - `DataLoss`: This is the primary loss function that measures how well the model predicts the target values for the given data. Examples include mean squared error (MSE) or cross-entropy loss.\n",
    " - `L_{1w}`, `L_{1b}`, `L_{2w}`, and `L_{2b}`: These are the regularization terms discussed earlier. They are added to the DataLoss to control the complexity of the model and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c790f2-48c3-4693-b250-f3e1e500a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_loss(weights, biases, data_loss, lambda_l1w, lambda_l1b, lambda_l2w, lambda_l2b):\n",
    "    \"\"\"\n",
    "        Calculates the total loss, including data loss and L1/L2 regularization for weights and biases.\n",
    "        \n",
    "        Args:\n",
    "          weights: A list or array of weights.\n",
    "          biases: A list or array of biases.\n",
    "          data_loss: The data loss (e.g., mean squared error, cross-entropy).\n",
    "          lambda_l1w: Regularization strength for L1 weight penalty.\n",
    "          lambda_l1b: Regularization strength for L1 bias penalty.\n",
    "          lambda_l2w: Regularization strength for L2 weight penalty.\n",
    "          lambda_l2b: Regularization strength for L2 bias penalty.\n",
    "        \n",
    "        Returns:\n",
    "          The total loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    l1w = lambda_l1w * sum(abs(w) for w in weights)\n",
    "    l1b = lambda_l1b * sum(abs(b) for b in biases)\n",
    "    l2w = lambda_l2w * sum(w**2 for w in weights)\n",
    "    l2b = lambda_l2b * sum(b**2 for b in biases)\n",
    "    loss = data_loss + l1w + l1b + l2w + l2b\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d03b8ef-4475-4533-aac0-2dd5f4666c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 0.328855\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming you have the following values:\n",
    "weights = [0.5, -1.2, 0.8]\n",
    "biases = [0.2, -0.1]\n",
    "data_loss = 0.3\n",
    "lambda_l1w = 0.01\n",
    "lambda_l1b = 0.005\n",
    "lambda_l2w = 0.001\n",
    "lambda_l2b = 0.0005\n",
    "\n",
    "total_loss = calculate_loss(weights, biases, data_loss, lambda_l1w, lambda_l1b, lambda_l2w, lambda_l2b)\n",
    "\n",
    "print(\"Total Loss:\", total_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f9262-9e9a-42b0-9ce3-c921f7d30134",
   "metadata": {},
   "source": [
    "The overall loss function is a combination of the data loss and regularization terms. By minimizing this overall loss, the model learns to make accurate predictions while keeping its weights and biases relatively small. This helps to improve the model's generalization ability and prevent it from overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad1d83-655c-4ceb-a79f-72a7f9c360f0",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Backward pass***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2de9-5fc2-44d3-af4c-af329f283dab",
   "metadata": {},
   "source": [
    "The derivative of L2 regularization is relatively simple:\n",
    "\n",
    "$$L_{2w} = \\lambda \\sum_{m} w_{m}^2$$\n",
    "\n",
    "$$\\frac{∂L_{2w}}{∂w_m} = λ \\frac{∂}{∂w_m} \\left[ \\sum_{m} w_{m}^2 \\right] = λ \\sum_{m} \\frac{∂}{∂w_m} \\left[ w_{m}^2 \\right] = λ \\sum_{m} 2w_m = \\frac{∂L_{2w}}{∂w_m} = 2λw_m$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0e103-47ab-45f8-95af-135689382fad",
   "metadata": {},
   "source": [
    "This might look complicated, so we can move it outside of the derivative term. We can remove the sum operator since we calculate the partial derivative with respect to the given parameter only, and the sum of one element equals this element. So, we only need to calculate the derivative of w​2​, which we know is 2w​. From the coding perspective, we will multiply all of the weights by 2λ​. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f8b45-bdec-4120-a744-87d95ff0f5bd",
   "metadata": {},
   "source": [
    "L1 regularization’s derivative, on the other hand, requires more explanation. In the case of L1 rgularization, we must calculate the derivative of the absolute value piecewise function, which effectively multiplies a value by -1 if it is less than 0; otherwise, it’s multiplied by 1. This is because the absolute value function is linear for positive values, and we know that a linear function’s derivative is: \n",
    "\n",
    "$$f(x) = x \\rightarrow f'(x) = 1$$\n",
    "For negative values, it negates the sign of the value to make it positive. In other words, it multiplies values by -1: \n",
    "$$f(x) = -x \\rightarrow f'(x) = -1$$\n",
    "When we combine that:\n",
    "\n",
    "$$abs(x) = \\begin{cases}\n",
    "    x & x > 0 \\\\\n",
    "    -x & x < 0\n",
    "\\end{cases} \\rightarrow abs'(x) = \\begin{cases}\n",
    "    1 & x > 0 \\\\\n",
    "    -1 & x < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "And the complete partial derivative of L1 regularization with respect to given weight:\n",
    "\n",
    "$$L_{1w} = \\lambda \\sum_{m} |w_m| \\rightarrow \\frac{∂L_{1w}}{∂w_m} = \\lambda \\frac{∂}{∂w_m} \\sum_{m} |w_m| = \\lambda \\frac{∂}{∂w_m} |w_m| = \\lambda \\begin{cases}\n",
    "    1 & w_m > 0 \\\\\n",
    "    -1 & w_m < 0\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a15302-4359-4fe0-bd5a-ddd24321ae7e",
   "metadata": {},
   "source": [
    "Like L2 regularization, **lambda** is a constant, and we calculate the partial derivative of this regularization with respect to the specific input. The partial derivative, in this case, equals 1 or -1 depending on the wm (weight) value. \n",
    "     \n",
    "We are calculating this derivative with respect to weights, and the resulting gradient, which has the same shape as the weights, is what we’ll use to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9787d-e3b0-4f6c-a8eb-105affb6397c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
