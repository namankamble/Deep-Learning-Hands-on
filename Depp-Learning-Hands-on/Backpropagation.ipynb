{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87f6ebf-e270-4bf5-9ebc-16b90b2f5291",
   "metadata": {},
   "source": [
    "***\n",
    "# <center>***Backpropagation***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe9ab42-c5a7-42a3-ad0a-e8ca13e37c91",
   "metadata": {},
   "source": [
    "In deep learning, backpropagation is a gradient estimation method commonly used for training a neural network to compute its parameter updates.\n",
    "\n",
    "It is an efficient application of the **chain rule** to neural networks. **Backpropagation** computes the **gradient** of a **loss function** with respect to the weights of the network for a single input‚Äìoutput example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.\n",
    "\n",
    "the term **backpropagation** refers only to an algorithm for **efficiently computing the gradient**, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm including how the gradient is used, such as by stochastic gradient descent, or as an intermediate step in a more complicated optimizer, such as Adaptive Moment Estimation. The local minimum convergence, exploding gradient, vanishing gradient, and weak control of learning rate are main disadvantages of these optimization algorithms. The Hessian and quasi-Hessian optimizers solve only local minimum convergence problem, and the backpropagation works longer. These problems caused researchers to develop hybrid and fractional optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8f38c-38fb-4974-850d-da1800e835fa",
   "metadata": {},
   "source": [
    "***\n",
    "## **How It Works**\n",
    "***\n",
    "\n",
    "**Initialization:** \n",
    "  - When a neural network is initialized, weights are often set to small random values.\n",
    "  - The network processes inputs through layers to generate predictions.\n",
    "\n",
    "**Forward Pass:**\n",
    "  - In this step, the input data is fed through the network layer by layer.\n",
    "  - Each neuron performs a weighted sum of its inputs and passes the result through an activation function to produce an output.\n",
    "  - This process continues until the final output is produced.\n",
    "\n",
    "**Loss Calculation:**\n",
    "  - The loss (or cost) function measures the difference between the predicted output and the actual output (e.g., mean squared error for regression tasks or cross-entropy loss for classification tasks).\n",
    "  - This loss value quantifies how well or poorly the network is performing.\n",
    "\n",
    "**Backward Pass (Backpropagation):**\n",
    "  - The goal of backpropagation is to reduce the loss.\n",
    "  - The algorithm calculates the gradient (partial derivatives) of the loss function with respect to each weight by the chain rule of calculus (this is why it's also known as gradient descent).\n",
    "\n",
    "**Step-by-Step of Backward Pass:**\n",
    "  - **Compute Gradients for Output Layer:**\n",
    "     - For the output layer neurons, compute the gradient of the loss concerning the neuron‚Äôs output. This tells you how much the loss would change if you changed the output of that neuron.\n",
    "\n",
    "  - **Propagate Gradients Backwards:**\n",
    "    - For each layer, starting from the output layer and moving backwards, compute the gradient of the loss with respect to each weight.\n",
    "    - This involves computing the product of the gradient with respect to the neuron‚Äôs output and the gradient of the neuron‚Äôs output with respect to the weight.\n",
    "\n",
    "  - **Update Weights:**\n",
    "      - Use the computed gradients to update the weights. This is typically done using an optimization algorithm like Stochastic Gradient Descent (SGD):\n",
    "        \n",
    "$$ùë§_{ùëõùëíùë§}=ùë§_{ùëúùëôùëë}‚àí ùúÇ \\cdot \\frac{‚àÇL}{‚àÇw}$$\n",
    "\n",
    "\n",
    "Here, \n",
    "- **ùë§** is the weight, \n",
    "- **ùúÇ** is the learning rate, and \n",
    "- **ùêø** is the loss function.\n",
    "\n",
    "By adjusting the weights slightly in the direction that reduces the loss, the network learns to produce more accurate outputs.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95050a3e-ff63-4f51-a691-917d7d38b370",
   "metadata": {},
   "source": [
    "Now that we have an idea of how to measure the impact of variables on a function‚Äôs output, we can begin to write the code to calculate these **partial derivatives** to see their role in minimizing the model‚Äôs loss. Before applying this to a complete neural network, let‚Äôs start with a simplified **forward pass** with just **one neuron**. Rather than **backpropagating** from the **loss function** for a full neural network, let‚Äôs **backpropagate** the **ReLU function** for a **single neuron** and act as if we intend to **minimize the output for this single neuron**. We are first doing this only as a demonstration to simplify the explanation, since minimizing the output from a ReLU activated neuron does not serve any purpose other than as an exercise. **Minimizing the loss value is our end goal**, but in this case, we will start by showing how we can leverage the **chain rule** with **derivatives and partial derivatives** to calculate the impact of each variable on the ReLU activated output. We will also start by minimizing this more basic output before jumping to the full network and overall loss. \n",
    "\n",
    "we need to perform for this **single neuron and ReLU activation**. We will use an example neuron with 3 inputs, which means that it also has 3 weights and a bias: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22a8810-f41d-4952-9023-5330f5316cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Forward pass \n",
    "x = [1.0, -2.0, 3.0]  # input values \n",
    "w = [-3.0, -1.0, 2.0]  # weights \n",
    "b = 1.0  # bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cfad74-e407-4e7b-827f-d2769ef416ed",
   "metadata": {},
   "source": [
    "We then start with the first input, x[0], and the related weight, w[0]:\n",
    "$$x[0] = 1.0$$\n",
    "$$w[0] = -3.0$$\n",
    "We have to multiply the input by the weight: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dafa001-4559-4b76-9695-9715a3b8613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 2.0 6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Multiplying inputs by weights \n",
    "xw0 = x[0] * w[0] \n",
    "xw1 = x[1] * w[1] \n",
    "xw2 = x[2] * w[2] \n",
    "print(xw0, xw1, xw2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd6c03-2e7f-4a3a-88da-6e54cdf0e8b1",
   "metadata": {},
   "source": [
    "The next operation to perform is a **sum of all weighted inputs with a bias**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58af67f9-39c3-46e5-aafe-61e813e5f321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Adding weighted inputs and a bias \n",
    "z = xw0 + xw1 + xw2 + b \n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc724935-47bb-49a9-870e-3dea64c11ad3",
   "metadata": {},
   "source": [
    "This forms the **neuron‚Äôs output**. The last step is to apply the **ReLU activation function** on this output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43cad69-bd22-4f66-b869-7f444ea64fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ReLU activation function \n",
    "y = max(z, 0) \n",
    "print(y) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362840d-5153-4d8a-8102-bd248a735b43",
   "metadata": {},
   "source": [
    "This is the full forward pass through a single neuron and a ReLU activation function. Let‚Äôs treat all of these chained functions as one big function which takes **input values (x‚Äã)**, **weights (w‚Äã)**, and **bias (b‚Äã)**, as **inputs, and outputs y‚Äã**. This big function consists of multiple simpler functions there is a multiplication of input values and weights, sum of these values and bias, as well as a max function as the ReLU activation 3 chained functions in total: The first step is to **backpropagate** our **gradients** by calculating derivatives and partial derivatives with respect to each of our parameters and inputs. To do this, we are going to use the **chain rule**. Recall that the chain rule for a function stipulates that the derivative for nested functions like **f(g(x))** solves to:\n",
    "\n",
    "$$\\frac{d}{dx}f(g(x)) = \\frac{d}{dg(x)}f(g(x)) \\cdot \\frac{d}{dx}g(x) = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "This big function that we just mentioned can be, in the context of our neural network, loosely interpreted as: \n",
    "\n",
    "$$ReLU(\\sum_{i}[inputs \\cdot weights] + bias)$$\n",
    "\n",
    "Or in the form that matches code more precisely as:\n",
    "\n",
    "$$ReLU(x_0w_0 + x_1w_1 + x_2w_2 + b)$$\n",
    "\n",
    "Our current task is to calculate how much each of the inputs, weights, and a bias impacts the output. We will start by considering what we need to calculate for the partial derivative of w‚Äã0‚Äã, for example. But first, let‚Äôs rewrite our equation to the form that will allow us to determine how to calculate the derivatives more easily: \n",
    "\n",
    "$$y = ReLU(sum(mul(x_v, w_v), mul(x_p, w_p), mul(x_z, w_z), b)))$$\n",
    "\n",
    "The above equation contains 3 nested functions: ReLU‚Äã, a sum of weighted inputs and a bias, and multiplications of the inputs and weights. To calculate the impact of the example weight, w‚Äã0‚Äã, on the output, the chain rule tells us to calculate the derivative of ReLU‚Äã with respect to its parameter, which is the sum, then multiply it with the partial derivative of the sum operation with respect to its mul(x‚Äã0‚Äã, w‚Äã0‚Äã)‚Äã input, as this input contains the parameter in question. Then, multiply this with the partial derivative of the multiplication operation with respect to the x‚Äã0‚Äã input. Let‚Äôs see this in a simplified equation:\n",
    "\n",
    "$$‚àÇ/‚àÇx‚ÇÄ [ReLU(sum(mul(x‚ÇÄ, w‚ÇÄ), mul(x‚ÇÅ, w‚ÇÅ), mul(x‚ÇÇ, w‚ÇÇ), b))] = \n",
    "dReLU()/dsum() * dsum()/dmul(x‚ÇÄ, w‚ÇÄ) * dmul(x‚ÇÄ, w‚ÇÄ)/dx‚ÇÄ$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe86113-45e8-49e2-90e5-1ba91526606d",
   "metadata": {},
   "source": [
    "During the backward pass, we will calculate the derivative of the loss function, and use it to multiply with the derivative of the activation function of the output layer, then use this result to multiply by the derivative of the output layer, and so on, through all of the hidden layers and activation functions. Inside these layers, the derivative with respect to the weights and biases will form the gradients that we will use to update the weights and biases. The derivatives with respect to inputs will form the gradient to chain with the previous layer. This layer can calculate the impact of its weights and biases on the loss and backpropagate gradients on inputs further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba4c02-161d-4abe-908e-dc89a08a8d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
