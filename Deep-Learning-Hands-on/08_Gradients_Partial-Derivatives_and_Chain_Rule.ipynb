{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c305a281-dc63-4594-a791-8bca75539535",
   "metadata": {},
   "source": [
    "***\n",
    "# <center>***Gradients, Partial Derivatives, and the Chain Rule***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8ed87-0f71-48a9-91ef-2f903b2acdfe",
   "metadata": {},
   "source": [
    "The **derivatives** that we have solved so far have been cases where there is only **one independent variable** in the function that is, the result depended solely on, in our case, x​. However, our neural network consists, for example, of neurons, which have multiple inputs. Each input gets multiplied by the corresponding weight (a function of 2 parameters), and they get summed with the bias (a function of as many parameters as there are inputs, plus one for a bias). To learn the impact of all of the inputs, weights, and biases to the neuron output and at the end of the loss function, we need to calculate the derivative of each operation performed during the forward pass in the neuron and the whole model. To do that and get answers, we will need to use the **chain rule**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff4bfc-a892-402e-99d5-0501d6ecb501",
   "metadata": {},
   "source": [
    "***\n",
    "### ***The Partial Derivative***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e4efc-e7d9-4c2e-9712-ada86e340c84",
   "metadata": {},
   "source": [
    "The **partial derivative** measures how much impact a single input has on a function’s output. The method for calculating a partial derivative is the same as for derivatives explained in the derivative file, we simply have to repeat this process for each of the independent inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84ff3e-1c70-4d5d-96fe-a6418308e6e0",
   "metadata": {},
   "source": [
    "Each of the function’s inputs has some impact on this function’s output, even if the impact is 0. We need to know these impacts, this means that we have to calculate the derivative with respect to each input separately to learn about each of them. That’s why we call these **partial derivatives** with respect to given input we are calculating a partial of the derivative, related to a singular\n",
    "input. **The partial derivative is a single equation**, and the full multivariate function’s derivative consists of a set of equations called the **gradient**. In other words, the **gradient** is a vector of the size of inputs containing partial derivative solutions with respect to each of the inputs.\n",
    "\n",
    "To denote the **partial derivative**, we will be using **Euler’s notation**. It’s very similar to Leibniz’s notation, as we only need to replace the differential operator *d*​** with **∂**. While the **d​** operator might be used to denote the differentiation of a multivariate function, its meaning is a bit different it can mean the rate of the function’s change in relation to the given input, but when other inputs might change as well, and it is used mostly in physics. We are interested in the partial derivatives, a situation where we try to find the impact of the given input to the output while treating all of the other inputs as constants. We are interested in the impact of singular inputs since our goal, in the model, is to update parameters. The **∂** operator means explicitly that the **partial derivative:**\n",
    "\n",
    "$$ f(x, y, z) = \\frac{∂}{∂x}f(x, y, z) = \\frac{∂}{∂y}f(x, y, z) = \\frac{∂}{∂z}f(x, y, z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc43d3-10d8-4479-8d42-83b561fb7527",
   "metadata": {},
   "source": [
    "***\n",
    "### ***The Partial Derivative of a Sum***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fb8e7-6c01-4396-80d8-236220b9d306",
   "metadata": {},
   "source": [
    "Calculating the partial derivative with respect to a given input means to calculate it like the \n",
    "regular derivative of one input, just while treating other inputs as constants. For example:\n",
    "\n",
    "$$ f(x, y) = x + y$$\n",
    "\n",
    "$$ \\frac{∂}{∂x}f(x, y) = \\frac{∂}{∂x}[x + y] = \\frac{∂}{∂x}x + \\frac{∂}{∂x}y = 1 + 0 = 1$$\n",
    "\n",
    "$$ \\frac{∂}{∂y}f(x, y) = \\frac{∂}{∂y}[x + y] = \\frac{∂}{∂y}x + \\frac{∂}{∂y}y = 0 + 1 = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44173d71-badf-4e5e-9b90-eb0e256ee5ea",
   "metadata": {},
   "source": [
    "***\n",
    "### ***The Partial Derivative of Multiplication***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966b344-23ec-4de5-97a0-748f7c609a2a",
   "metadata": {},
   "source": [
    "$$ f(x, y) = x \\cdot y$$\n",
    "\n",
    "$$ \\frac{∂}{∂x}f(x, y) = \\frac{∂}{∂x}(x \\cdot y) = y \\cdot \\frac{∂}{∂x}x = y \\cdot 1 = y$$\n",
    "\n",
    "$$ \\frac{∂}{∂y}f(x, y) = \\frac{∂}{∂y}(x \\cdot y) = x \\cdot \\frac{∂}{∂y}y = x \\cdot 1 = x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a0b2a-4aae-4b28-a0f1-54df1fb67203",
   "metadata": {},
   "source": [
    "***\n",
    "### ***The Partial Derivative of Max***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a46ec-9aa8-40fb-8101-dcd5bcf7193d",
   "metadata": {},
   "source": [
    "Derivatives and partial derivatives are not limited to addition and multiplication operations, or\n",
    "constants. We need to derive them for the other functions that we used in the forward pass, one of \n",
    "which is the derivative of the max()​ function: \n",
    "\n",
    "$$f(x, y) = max(x, y)$$\n",
    "\n",
    "$$\\frac{∂}{∂x}f(x, y) = \\begin{cases}\n",
    "1 & \\text{if } x > y \\\\\n",
    "0 & \\text{if } x < y \\\\\n",
    "\\text{undefined} & \\text{if } x = y\n",
    "\\end{cases}$$\n",
    "\n",
    "The max function returns the greatest input. We know that the derivative of x​ with respect to x \n",
    "equals 1, ​ so the derivative of this function with respect to x​ equals 1 if x ​ is greater than y​, since the function will return x​. In the other case, where y​ is greater than x​ and will get returned instead, the derivative of **max()​** with respect to x​ equals 0 we treat y​ as a constant, and the derivative of y with respect to x​ equals 0. We can denote that as 1(x > y)​, which means 1​ if the condition is met, and 0​ otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e6097-2bb4-408f-a022-e5e90bc4233a",
   "metadata": {},
   "source": [
    "***\n",
    "### ***The Gradient***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbfcd5-8ea2-4ea4-b9e8-7bb75f620919",
   "metadata": {},
   "source": [
    "As we mentioned, the gradient is a vector composed of all of the partial derivatives of a unction, calculated with respect to each input variable.\n",
    "\n",
    "$$f(x, y, z) = 3x^3z - y^2 + 5z + 2yz$$\n",
    "\n",
    "$$\\frac{∂}{∂x}f(x, y, z) = 9x^2z$$\n",
    "$$\\frac{∂}{∂y}f(x, y, z) = -2y + 2z$$\n",
    "$$\\frac{∂}{∂z}f(x, y, z) = 3x^3 + 5 + 2y$$\n",
    "\n",
    "If we calculate all of the partial derivatives, we can form a gradient of the function. Using different notations, it looks as follows:\n",
    "\n",
    "$$\\nabla f(x, y, z) = \\begin{bmatrix}\n",
    "    \\frac{∂}{∂x}f(x, y, z) \\\\\n",
    "    \\frac{∂}{∂y}f(x, y, z) \\\\\n",
    "    \\frac{∂}{∂z}f(x, y, z) \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{∂}{∂x} \\\\\n",
    "    \\frac{∂}{∂y} \\\\\n",
    "    \\frac{∂}{∂z} \n",
    "\\end{bmatrix} f(x, y, z) = \\begin{bmatrix}\n",
    "    9x^2z \\\\\n",
    "    -2y + 2z \\\\\n",
    "    3x^3 + 5 + 2y\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "That’s all we have to know about the **gradient** it’s a vector of all of the possible partial \n",
    "derivatives of the function, and we denote it using the ∇ nabla symbol that looks like an inverted delta symbol. \n",
    "\n",
    "We will be using **derivatives** of single-parameter functions and **gradients** to perform **gradient descent** using the **chain rule**, of multivariate functions or, in other words, to perform the **backward pass**, which is a part of the model training. How exactly we’ll do that is the subject of the next chapter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a2752-9183-4328-9da6-4eeb9162667d",
   "metadata": {},
   "source": [
    "***\n",
    "### ***The Chain Rule***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e82e0-8f63-4b4c-a258-283677f8ca0e",
   "metadata": {},
   "source": [
    "During the **forward pass**, we are passing the data through the neurons, then through the activation function, then through the neurons in the next layer, then through another activation function, and so on. We are calling a function with an input parameter, taking an output, and using that output as an input to another function. For this simple example, let’s take 2 functions: **f​** and **g​**\n",
    "\n",
    "$$z = f(x)$$\n",
    "$$y = g(z)$$\n",
    "\n",
    "x is the input data, z is an output of the function f, but also an input for the function g, and y is an output of the function g. We could write the same calculation as: \n",
    "\n",
    "$$y = g(f(x))$$\n",
    "\n",
    "In this form, we do not use the intermediate z variable, showing that function g takes the output of function f directly as an input. This does not differ much from the above 2 equations but shows an important property of functions chained this way — since x is an input to the function f and then the output of the function f is an input to the function g, the output of the function g is influenced by x in some way, so there must exist a derivative which can inform us of this influence. The forward pass through our model is a chain of functions similar to these examples. We are passing in samples, the data flows through all of the layers, and activation functions to form an output. Let’s bring the equation and the code of the example model from chapter 1: \n",
    "\n",
    "$$L = -\\sum_{i=1}^{N} y_i \\log \\left( \\frac{e^{z_i} \\prod_{j=1}^{n_i} \\left( 1 + \\sum_{k=1}^{m_i} \\max(0, f_i^k(x_{i,j})) \\right) \\prod_{j=1}^{n_i} \\max(0, f_i^0(x_{i,j})) \\sum_{j=1}^{n_i} \\sum_{k=1}^{m_i} w_{i,j,k} \\cdot x_{i,j,k} + b_i}{\\sum_{l=0}^{n_i} e^{z_l} \\prod_{j=1}^{n_i} \\left( 1 + \\sum_{k=1}^{m_i} \\max(0, f_l^k(x_{l,j})) \\right) \\prod_{j=1}^{n_i} \\max(0, f_l^0(x_{l,j})) \\sum_{j=1}^{n_i} \\sum_{k=1}^{m_i} w_{l,j,k} \\cdot x_{l,j,k} + b_l} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdb6e2-4667-4337-98a8-41f6fde56e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
