{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66cc7b75-1cb4-400c-b6d2-9f814e9a9453",
   "metadata": {},
   "source": [
    "***\n",
    "# ***Optimizers in Deep Learning***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b085e519-e283-41c8-8677-c85dd36530a8",
   "metadata": {},
   "source": [
    "In deep learning, an optimizer is an algorithm that adjusts a neural network's parameters to minimize the loss function. This process is crucial for the model to learn effectively. It involves making small, incremental changes to the parameters, but finding the right balance is tricky. An optimizer in a neural network helps guide the model to the best solution. Without them, the model might struggle to converge or learn effectively. The challenge 1  of choosing the right weights for the model is a daunting task, as a deep learning model generally consists of millions of parameters. 2  ¬† \n",
    "\n",
    "These specialized algorithms facilitate the learning process of neural networks by iteratively refining the **weights** and **biases** based on the feedback received from the data. Well known optimizers in deep learning encompass **Stochastic Gradient Descent (SGD)**, **Adam**, and **RMSprop**, each equipped with **distinct update rules**, **learning rates**, and **momentum strategies**, all geared towards the overarching goal of discovering and converging upon optimal model parameters, thereby enhancing overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad48fc1-c37c-4b34-8a11-70227a516248",
   "metadata": {},
   "source": [
    "***\n",
    "### ***Important Deep Learning Terms***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456faab-9938-4f3c-b604-663a8d0c91a6",
   "metadata": {},
   "source": [
    "**Before proceeding, there are a few terms that you should be familiar with.**\n",
    "\n",
    "- `Epoch` ‚Äì The number of times the algorithm runs on the whole training dataset.\n",
    "- `Sample` ‚Äì A single row of a dataset.\n",
    "- `Batch` ‚Äì It denotes the number of samples to be taken to for updating the model parameters.\n",
    "- `Learning rate` ‚Äì It is a parameter that provides the model a scale of how much model weights should be updated.\n",
    "- `Cost Function/Loss Function` ‚Äì A cost function is used to calculate the cost, which is the difference between the predicted value and the actual value.\n",
    "- `Weights/ Bias` ‚Äì The learnable parameters in a model that controls the signal between two neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002096fe-13a2-4d97-8c46-107ba61e9325",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d336c-1f32-49ce-9338-73f8c2e9113d",
   "metadata": {},
   "source": [
    "***\n",
    "### ***Gradient Descent Deep Learning Optimizer***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10bd32-5df8-4d05-a86c-38e377f07793",
   "metadata": {},
   "source": [
    "This optimization algorithm uses calculus to consistently modify the values and achieve the local minimum. Before moving ahead, you might question what a gradient is.\n",
    "\n",
    "In simple terms, consider you are holding a ball resting at the top of a bowl. When you lose the ball, it goes along the steepest direction and eventually settles at the bottom of the bowl. A Gradient provides the ball in the steepest direction to reach the local minimum which is the bottom of the bowl.\n",
    "\n",
    "Gradient descent works best for most purposes. However, it has some downsides too. It is expensive to calculate the gradients if the size of the data is huge. Gradient descent works well for convex functions, but it doesn‚Äôt know how far to travel along the gradient for nonconvex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf30efb-9309-4bc9-bde0-9ffbeeb465ea",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105587a1-9adf-4498-801f-3e8d59328734",
   "metadata": {},
   "source": [
    "***\n",
    "### ***Stochastic Gradient Descent Deep Learning Optimizer***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bb450-4e68-4468-af5f-3c19dbd69eee",
   "metadata": {},
   "source": [
    "To tackle the challenges large datasets pose, we have stochastic gradient descent, a popular approach among optimizers in deep learning. The term stochastic denotes the element of randomness upon which the algorithm relies. In stochastic gradient descent, instead of processing the entire dataset during each iteration, we randomly select batches of data. This implies that only a few samples from the dataset are considered at a time, allowing for more efficient and computationally feasible optimization in deep learning models.\n",
    "$$\n",
    "     W = W - \\eta \\cdot \\nabla L(W)\n",
    "     $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e4674d-ed7a-4243-bd44-42bc0e330a37",
   "metadata": {},
   "source": [
    "The procedure is first to select the initial parameters w and learning rate n. Then randomly shuffle the data at each iteration to reach an approximate minimum.\n",
    "\n",
    "Since we are not using the whole dataset but the batches of it for each iteration, the path taken by the algorithm is full of noise as compared to the gradient descent algorithm. Thus, SGD uses a higher number of iterations to reach the local minima. Due to an increase in the number of iterations, the overall computation time increases. But even after increasing the number of iterations, the computation cost is still less than that of the gradient descent optimizer. So the conclusion is if the data is enormous and computational time is an essential factor, stochastic gradient descent should be preferred over batch gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba3e1b-19da-470b-97f2-20a6b846dfa5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498c4eb-4a16-4273-9a81-a2174eb23780",
   "metadata": {},
   "source": [
    "***\n",
    "### ***Stochastic Gradient Descent With Momentum Deep Learning Optimizer***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be6f31-04f4-46e9-8797-d91a5553d13c",
   "metadata": {},
   "source": [
    "*Stochastic Gradient Descent (SGD) with Momentum is an advanced version of the basic SGD optimizer. It aims to speed up the learning process by considering the previous updates while computing the current update. Here‚Äôs how it works:*\n",
    "\n",
    "**Stochastic Gradient Descent (SGD):**\n",
    "\n",
    "Imagine you're trying to roll a ball to the lowest point in a hilly terrain. With basic SGD, you take small steps in the direction of the steepest descent. However, this process can be slow and can get stuck in local minima (small dips that are not the lowest point).\n",
    "\n",
    "**Adding Momentum:**\n",
    "\n",
    "Momentum helps overcome these issues by adding a fraction of the previous update to the current update. This means the optimizer not only relies on the current gradient but also takes into account the direction of the previous step.\n",
    "\n",
    "Think of it as adding inertia to the ball, so it keeps rolling in the same direction unless a significant force (gradient) changes its path.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "Faster Convergence: By carrying forward the inertia, it helps the optimizer move faster towards the global minimum.\n",
    "\n",
    "Smoother Updates: It reduces oscillations and smooths out the updates, leading to more stable training.\n",
    "\n",
    "Here's a simplified formula for SGD with Momentum: $$ v_{t+1} = \\beta v_t - \\alpha \\nabla f(\\theta_t) $$ $$ \\theta_{t+1} = \\theta_t + v_{t+1} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- ùë£\n",
    "ùë°\n",
    " is the velocity (momentum) at time step t\n",
    "\n",
    "- ùõΩ\n",
    " is the momentum factor (typically between 0.9 and 0.99)\n",
    "\n",
    "- ùõº\n",
    " is the learning rate\n",
    "\n",
    "- ‚àá\n",
    "ùëì\n",
    "(\n",
    "ùúÉ\n",
    "ùë°\n",
    ")\n",
    " is the gradient of the loss function with respect to the parameters at time step t\n",
    "\n",
    "- ùúÉ\n",
    "ùë°\n",
    " are the parameters at time step t\n",
    "\n",
    "In essence, SGD with Momentum helps navigate the optimization landscape more efficiently, ensuring that the model learns quickly and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853af35b-b44e-42ca-9ced-ab767abe94fb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72974d30-e4cf-4ec0-aaf6-0dededfafe7e",
   "metadata": {},
   "source": [
    "***\n",
    "### ***Mini Batch Gradient Descent Deep Learning Optimizer***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b840f1-3e8e-4564-b318-9dfe7bbffaa4",
   "metadata": {},
   "source": [
    "*Mini Batch Gradient Descent is a popular optimization technique in deep learning that strikes a balance between two other methods: Stochastic Gradient Descent (SGD) and Batch Gradient Descent.*\n",
    "\n",
    "**Gradient Descent Methods:**\n",
    "\n",
    "`Batch Gradient Descent:` This method computes the gradient of the loss function with respect to the parameters for the entire dataset. While it's accurate, it can be very slow and computationally expensive, especially with large datasets.\n",
    "\n",
    "`Stochastic Gradient Descent (SGD):` This method updates the parameters using the gradient computed from a single data point at each iteration. It's faster but can be noisy and less stable.\n",
    "\n",
    "`Mini Batch Gradient Descent:` This method takes the best of both worlds by updating the parameters using the gradient computed from a small, random subset of the dataset called a mini-batch. It offers a compromise between the stability of batch gradient descent and the speed of stochastic gradient descent.\n",
    "\n",
    "**Advantages of Mini Batch Gradient Descent:**\n",
    "\n",
    "`Efficiency:` It uses mini-batches, which fit well with modern hardware like GPUs, making the training process faster.\n",
    "\n",
    "`Stability:` By averaging the gradients over a mini-batch, it reduces the noise compared to SGD, leading to more stable updates.\n",
    "\n",
    "`Scalability:` It works well with large datasets, as it does not require loading the entire dataset into memory at once.\n",
    "\n",
    "**Here's a simplified outline of how Mini Batch Gradient Descent works:**\n",
    "\n",
    "- Randomly shuffle the dataset.\n",
    "\n",
    "- Divide the dataset into mini-batches of a fixed size.\n",
    "\n",
    "- For each mini-batch, compute the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "- Update the parameters using the average gradient of the mini-batch.\n",
    "\n",
    "- Repeat the process for a set number of iterations or until the model converges.\n",
    "\n",
    "In essence, Mini Batch Gradient Descent helps neural networks learn more efficiently by making smart compromises between speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442361e-90f0-4774-8ab7-ea2cbbd44593",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e5c21-0f48-43d0-8e5b-70a0b9d13feb",
   "metadata": {},
   "source": [
    "***\n",
    "### ***Adagrad (Adaptive Gradient Descent) Deep Learning Optimizer***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a34b4-41b3-4720-ae6c-cbe03000c543",
   "metadata": {},
   "source": [
    "*Adagrad (Adaptive Gradient Descent) is a popular optimization algorithm in deep learning that adapts the learning rate for each parameter based on their historical gradients. This means that parameters with frequently large gradients get smaller learning rates, and parameters with infrequently large gradients get larger learning rates. Here's a simplified explanation:*\n",
    "\n",
    "**Key Concept:**\n",
    "\n",
    "Adagrad updates the learning rate for each parameter dynamically, which helps the model converge more quickly and efficiently.\n",
    "\n",
    "**How Adagrad Works:**\n",
    "\n",
    "`Initialization:` Like other optimizers, Adagrad starts by initializing the parameters and setting a learning rate.\n",
    "\n",
    "`Gradient Accumulation:` It keeps a running total of the squared gradients for each parameter.\n",
    "\n",
    "`Adaptive Learning Rate:` The learning rate for each parameter is adjusted based on the accumulated squared gradients. Parameters that have received large updates in the past will have smaller learning rates in the future.\n",
    "\n",
    "`Formula:` The update rule for Adagrad can be represented as: $$ \\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{G_{t,ii} + \\epsilon}} \\nabla f(\\theta_t) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- ùúÉ\n",
    "ùë°\n",
    " are the parameters at time step t\n",
    "\n",
    "- ùõº\n",
    " is the initial learning rate\n",
    "\n",
    "- ùê∫\n",
    "ùë°\n",
    ",\n",
    "ùëñ\n",
    "ùëñ\n",
    " is the sum of the squares of the gradients up to time step t for parameter \n",
    "ùëñ\n",
    "\n",
    "- ùúñ\n",
    " is a small smoothing term to avoid division by zero\n",
    "\n",
    "- ‚àá\n",
    "ùëì\n",
    "(\n",
    "ùúÉ\n",
    "ùë°\n",
    ")\n",
    " is the gradient of the loss function with respect to the parameters at time step t\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "`Automatic Learning Rate Adjustment:` Adagrad adjusts the learning rate for each parameter, making it more efficient and reducing the need to manually tune the learning rate.\n",
    "\n",
    "`Effective for Sparse Data:` It's particularly useful for models with sparse data or features, as it ensures that rare but important features are given appropriate attention during training.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "`Decaying Learning Rate:` Over time, the learning rate can become very small, causing the model to stop learning. This can sometimes be mitigated by using variants like Adadelta or RMSprop.\n",
    "\n",
    "In essence, Adagrad helps optimize the training process by dynamically adjusting the learning rates of parameters, ensuring efficient and effective convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff50b8c-744e-4a2b-b665-f7f6a79d50e9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c6206-9c85-458d-ae96-4df37b18f5f2",
   "metadata": {},
   "source": [
    "***\n",
    "### ***RMS Prop (Root Mean Square) Deep Learning Optimizer***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4446420-662c-470e-906b-d0f9d76e6df6",
   "metadata": {},
   "source": [
    "*RMSprop (Root Mean Square Propagation) is another popular optimization algorithm used in deep learning that improves upon Adagrad by addressing its main drawback: the decaying learning rate. RMSprop adapts the learning rate for each parameter by dividing the gradient by an exponentially decaying average of past squared gradients.*\n",
    "\n",
    "**Key Concept:**\n",
    "\n",
    "RMSprop maintains a moving average of the squared gradients for each parameter, allowing the learning rate to adapt over time without decaying too quickly.\n",
    "\n",
    "**How RMSprop Works:**\n",
    "\n",
    "`Initialization:` Like other optimizers, RMSprop starts by initializing the parameters and setting a learning rate.\n",
    "\n",
    "`Gradient Squaring:` It computes the square of the gradients for each parameter.\n",
    "\n",
    "`Moving Average:` It maintains an exponentially decaying average of the squared gradients.\n",
    "\n",
    "`Adaptive Learning Rate:` The learning rate for each parameter is adjusted based on the moving average, preventing the learning rate from becoming too small.\n",
    "\n",
    "`Formula:` The update rule for RMSprop can be represented as: $$ E[g^2]t = \\rho E[g^2]{t-1} + (1 - \\rho) g_t^2 $$ $$ \\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\epsilon}} g_t $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- ùê∏\n",
    "[\n",
    "ùëî\n",
    "2\n",
    "]\n",
    "ùë°\n",
    " is the exponentially decaying average of past squared gradients at time step t\n",
    "\n",
    "- ùúå\n",
    " is the decay rate (typically around 0.9)\n",
    "\n",
    "- ùëî\n",
    "ùë°\n",
    " is the gradient of the loss function with respect to the parameters at time step t\n",
    "\n",
    "- ùõº\n",
    " is the learning rate\n",
    "\n",
    "- ùúñ\n",
    " is a small smoothing term to avoid division by zero\n",
    "\n",
    "- ùúÉ\n",
    "ùë°\n",
    " are the parameters at time step t\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "`Prevents Decay:` By using an exponentially decaying average, RMSprop prevents the learning rate from becoming too small, allowing the model to continue learning effectively.\n",
    "\n",
    "`Efficient:` It works well with mini-batches, making it suitable for large datasets and modern hardware like GPUs.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "`Hyperparameter Tuning:` The decay rate (\n",
    "ùúå\n",
    ") and learning rate (\n",
    "ùõº\n",
    ") may require tuning for optimal performance.\n",
    "\n",
    "In summary, RMSprop is a powerful optimizer that adapts the learning rate for each parameter based on the moving average of past squared gradients, ensuring efficient and stable training in deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6d2c6-de5e-433a-b89b-46156f698c4e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659d43c-76b3-4da8-b39b-f4464621577c",
   "metadata": {},
   "source": [
    "***\n",
    "### ***AdaDelta Deep Learning Optimizer***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac680c-7c06-4ed6-a229-2996dba2343e",
   "metadata": {},
   "source": [
    "*AdaDelta is an advanced optimization algorithm that improves upon the limitations of Adagrad. Its main goal is to reduce the aggressive, monotonically decreasing learning rate problem of Adagrad, which can sometimes lead to very small updates and slow learning. AdaDelta achieves this by incorporating only a window of accumulated past gradients instead of all past gradients, and it dynamically adapts the learning rate for each parameter.*\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "`Adaptive Learning Rate:` AdaDelta adjusts the learning rate based on a moving window of gradient updates.\n",
    "\n",
    "`No Manual Learning Rate:` Unlike many other optimizers, AdaDelta does not require a manually set learning rate (\n",
    "ùõº\n",
    ").\n",
    "\n",
    "**How AdaDelta Works:**\n",
    "\n",
    "`Accumulated Gradient:` It keeps track of an exponentially decaying average of squared gradients.\n",
    "\n",
    "`Update Rule:` Instead of using the accumulated squared gradients, AdaDelta uses a running average of the squared gradients and updates the parameters accordingly.\n",
    "\n",
    "`Dynamic Learning Rate:` The learning rate is adjusted dynamically for each parameter without needing to set an initial learning rate.\n",
    "\n",
    "`Formula:` The update rule for AdaDelta can be represented as: $$ E[g^2]t = \\rho E[g^2]{t-1} + (1 - \\rho) g_t^2 $$ $$ \\Delta \\theta_t = -\\frac{\\sqrt{E[\\Delta \\theta^2]_{t-1} + \\epsilon}}{\\sqrt{E[g^2]t + \\epsilon}} g_t $$ $$ \\theta{t+1} = \\theta_t + \\Delta \\theta_t $$ $$ E[\\Delta \\theta^2]t = \\rho E[\\Delta \\theta^2]{t-1} + (1 - \\rho) \\Delta \\theta_t^2 $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- ùê∏\n",
    "[\n",
    "ùëî\n",
    "2\n",
    "]\n",
    "ùë°\n",
    " is the exponentially decaying average of past squared gradients at time step t\n",
    "\n",
    "- Œî\n",
    "ùúÉ\n",
    "ùë°\n",
    " is the parameter update at time step t\n",
    "\n",
    "- ùê∏\n",
    "[\n",
    "Œî\n",
    "ùúÉ\n",
    "2\n",
    "]\n",
    "ùë°\n",
    " is the exponentially decaying average of past squared updates\n",
    "\n",
    "- ùúå\n",
    " is the decay rate (typically around 0.95)\n",
    "\n",
    "- ùëî\n",
    "ùë°\n",
    " is the gradient of the loss function with respect to the parameters at time step t\n",
    "\n",
    "- ùúñ\n",
    " is a small smoothing term to avoid division by zero\n",
    "\n",
    "- ùúÉ\n",
    "ùë°\n",
    " are the parameters at time step t\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "`No Manual Learning Rate:` It eliminates the need to manually set a learning rate, simplifying the training process.\n",
    "\n",
    "`Adaptability:` The dynamic adjustment of learning rates allows for efficient training and quick convergence.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "`Hyperparameter Tuning:` The decay rate (\n",
    "ùúå\n",
    ") and smoothing term (\n",
    "ùúñ\n",
    ") may still require some tuning for optimal performance.\n",
    "\n",
    "In summary, AdaDelta is a robust optimizer that adapts the learning rate dynamically, ensuring efficient and stable training in deep learning models without the need for a manually set learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082aedac-f6f2-460f-ba40-0f1853e44c6e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a6994-7139-4eb8-b70f-49005def47f4",
   "metadata": {},
   "source": [
    "***\n",
    "### ***Adam Optimizer in Deep Learning***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b13de-c41f-4264-a1bc-4d8fb5594e6e",
   "metadata": {},
   "source": [
    "*Adam (short for Adaptive Moment Estimation) is a widely used and highly effective optimization algorithm in deep learning. It combines the best features of two other optimizers, AdaGrad and RMSProp, and is well-suited for a variety of deep learning tasks. Here's a breakdown of how Adam works and why it's so popular:*\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "`Adaptive Learning Rates:` Adam adjusts the learning rate for each parameter individually based on estimates of first and second moments of the gradients.\n",
    "\n",
    "`Momentum:` It incorporates the concept of momentum to improve convergence speed and stability.\n",
    "\n",
    "**How Adam Works:**\n",
    "\n",
    "`Initialization:` Adam starts by initializing parameters and setting up learning rates, along with two additional parameters for controlling the moving averages of the gradients.\n",
    "\n",
    "`First Moment Estimate (Mean):` Adam maintains an exponentially decaying average of past gradients (mean of the gradients).\n",
    "\n",
    "`Second Moment Estimate (Variance):` Adam also keeps an exponentially decaying average of past squared gradients (uncentered variance).\n",
    "\n",
    "`Bias Correction:` To correct the bias introduced during the initialization, Adam applies bias correction to the first and second moment estimates.\n",
    "\n",
    "`Parameter Update:` Finally, it updates the parameters using the corrected moment estimates.\n",
    "\n",
    "`Formula:` The update rule for Adam can be represented as: $$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$ $$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $$ $$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} $$ $$ \\hat{v}t = \\frac{v_t}{1 - \\beta_2^t} $$ $$ \\theta{t+1} = \\theta_t - \\frac{\\alpha \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- ùëö\n",
    "ùë°\n",
    " is the first moment (mean) estimate at time step t\n",
    "\n",
    "- ùë£\n",
    "ùë°\n",
    " is the second moment (variance) estimate at time step t\n",
    "\n",
    "- ùõΩ\n",
    "1\n",
    " and \n",
    "ùõΩ\n",
    "2\n",
    " are decay rates for the moment estimates (typically around 0.9 and 0.999, respectively)\n",
    "\n",
    "- ùëî\n",
    "ùë°\n",
    " is the gradient of the loss function with respect to the parameters at time step t\n",
    "\n",
    "- ùëö\n",
    "^\n",
    "ùë°\n",
    " and \n",
    "ùë£\n",
    "^\n",
    "ùë°\n",
    " are the bias-corrected moment estimates\n",
    "\n",
    "- ùõº\n",
    " is the learning rate\n",
    "\n",
    "- ùúñ\n",
    " is a small smoothing term to avoid division by zero\n",
    "\n",
    "- ùúÉ\n",
    "ùë°\n",
    " are the parameters at time step t\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "`Efficient:` Adam combines the advantages of both AdaGrad and RMSProp, making it suitable for a wide range of problems.\n",
    "\n",
    "`Adaptive:` It adapts the learning rate for each parameter, leading to faster convergence and improved performance.\n",
    "\n",
    "`Robust:` Adam is known for its robustness and ability to handle sparse gradients and noisy data.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "`Hyperparameter Sensitivity:` While Adam typically works well out-of-the-box, fine-tuning the hyperparameters (learning rate, decay rates) can sometimes be necessary for optimal performance.\n",
    "\n",
    "In summary, Adam is a powerful and versatile optimizer that adapts the learning rates for each parameter based on past gradient estimates, making it an excellent choice for training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa82cf8-d593-4854-abfb-f585be28d63b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e492f5-b08d-4682-a667-789b26776f61",
   "metadata": {},
   "source": [
    "***\n",
    "### ***Hands-on Optimizers***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c84cdbb7-8b05-4f9b-8ecd-1b5d2ce481f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer\n",
      "Epoch 1/5\n",
      "1688/1688 - 10s - 6ms/step - accuracy: 0.8240 - loss: 0.6885 - val_accuracy: 0.9153 - val_loss: 0.3240\n",
      "Epoch 2/5\n",
      "1688/1688 - 8s - 5ms/step - accuracy: 0.9025 - loss: 0.3522 - val_accuracy: 0.9265 - val_loss: 0.2607\n",
      "Epoch 3/5\n",
      "1688/1688 - 8s - 5ms/step - accuracy: 0.9158 - loss: 0.3014 - val_accuracy: 0.9373 - val_loss: 0.2319\n",
      "Epoch 4/5\n",
      "1688/1688 - 7s - 4ms/step - accuracy: 0.9241 - loss: 0.2707 - val_accuracy: 0.9422 - val_loss: 0.2116\n",
      "Epoch 5/5\n",
      "1688/1688 - 8s - 5ms/step - accuracy: 0.9304 - loss: 0.2476 - val_accuracy: 0.9473 - val_loss: 0.1955\n",
      "313/313 - 1s - 4ms/step - accuracy: 0.9339 - loss: 0.2284\n",
      "Test accuracy with SGD: 0.933899998664856\n",
      "\n",
      "Training with SGD with Momentum optimizer\n",
      "Epoch 1/5\n",
      "1688/1688 - 10s - 6ms/step - accuracy: 0.9401 - loss: 0.2047 - val_accuracy: 0.9630 - val_loss: 0.1340\n",
      "Epoch 2/5\n",
      "1688/1688 - 8s - 5ms/step - accuracy: 0.9611 - loss: 0.1336 - val_accuracy: 0.9712 - val_loss: 0.1016\n",
      "Epoch 3/5\n",
      "1688/1688 - 8s - 5ms/step - accuracy: 0.9707 - loss: 0.1002 - val_accuracy: 0.9745 - val_loss: 0.0915\n",
      "Epoch 4/5\n",
      "1688/1688 - 8s - 5ms/step - accuracy: 0.9758 - loss: 0.0811 - val_accuracy: 0.9708 - val_loss: 0.1051\n",
      "Epoch 5/5\n",
      "1688/1688 - 8s - 5ms/step - accuracy: 0.9809 - loss: 0.0666 - val_accuracy: 0.9785 - val_loss: 0.0802\n",
      "313/313 - 1s - 4ms/step - accuracy: 0.9727 - loss: 0.0866\n",
      "Test accuracy with SGD with Momentum: 0.9726999998092651\n",
      "\n",
      "Training with Adam optimizer\n",
      "Epoch 1/5\n",
      "1688/1688 - 11s - 7ms/step - accuracy: 0.9767 - loss: 0.0757 - val_accuracy: 0.9760 - val_loss: 0.0879\n",
      "Epoch 2/5\n",
      "1688/1688 - 9s - 5ms/step - accuracy: 0.9833 - loss: 0.0545 - val_accuracy: 0.9772 - val_loss: 0.0770\n",
      "Epoch 3/5\n",
      "1688/1688 - 10s - 6ms/step - accuracy: 0.9864 - loss: 0.0425 - val_accuracy: 0.9787 - val_loss: 0.0818\n",
      "Epoch 4/5\n",
      "1688/1688 - 9s - 5ms/step - accuracy: 0.9898 - loss: 0.0329 - val_accuracy: 0.9767 - val_loss: 0.0893\n",
      "Epoch 5/5\n",
      "1688/1688 - 9s - 5ms/step - accuracy: 0.9912 - loss: 0.0278 - val_accuracy: 0.9753 - val_loss: 0.0942\n",
      "313/313 - 1s - 4ms/step - accuracy: 0.9754 - loss: 0.0813\n",
      "Test accuracy with Adam: 0.9753999710083008\n",
      "\n",
      "Training with RMSprop optimizer\n",
      "Epoch 1/5\n",
      "1688/1688 - 11s - 6ms/step - accuracy: 0.9952 - loss: 0.0162 - val_accuracy: 0.9815 - val_loss: 0.0809\n",
      "Epoch 2/5\n",
      "1688/1688 - 10s - 6ms/step - accuracy: 0.9960 - loss: 0.0130 - val_accuracy: 0.9817 - val_loss: 0.0833\n",
      "Epoch 3/5\n",
      "1688/1688 - 9s - 5ms/step - accuracy: 0.9969 - loss: 0.0106 - val_accuracy: 0.9817 - val_loss: 0.0919\n",
      "Epoch 4/5\n",
      "1688/1688 - 9s - 5ms/step - accuracy: 0.9972 - loss: 0.0092 - val_accuracy: 0.9822 - val_loss: 0.0868\n",
      "Epoch 5/5\n",
      "1688/1688 - 9s - 5ms/step - accuracy: 0.9977 - loss: 0.0079 - val_accuracy: 0.9817 - val_loss: 0.0896\n",
      "313/313 - 1s - 4ms/step - accuracy: 0.9796 - loss: 0.0824\n",
      "Test accuracy with RMSprop: 0.9796000123023987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Create a simple neural network model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with different optimizers\n",
    "optimizers = {\n",
    "    \"SGD\": SGD(learning_rate=0.01),\n",
    "    \"SGD with Momentum\": SGD(learning_rate=0.01, momentum=0.9),\n",
    "    \"Adam\": Adam(learning_rate=0.001),\n",
    "    \"RMSprop\": RMSprop(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "# Train and evaluate the model with each optimizer\n",
    "for name, optimizer in optimizers.items():\n",
    "    print(f\"Training with {name} optimizer\")\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=2)\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(f\"Test accuracy with {name}: {accuracy}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9b304-96eb-41e7-84cb-73375ab654f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95b4ef-d697-45ca-8fb2-b4a48c33dffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
