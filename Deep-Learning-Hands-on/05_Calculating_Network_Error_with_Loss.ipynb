{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412052bf-7107-4629-81e1-eeb50bfaa6ab",
   "metadata": {},
   "source": [
    "***\n",
    "# ***Calculating Network Error with Loss***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ab4af-7e05-42d9-838b-d09c6b90593e",
   "metadata": {},
   "source": [
    "In deep learning, calculating network error involves evaluating how well the neural network's predictions match the true labels. This is where the concept of **loss** comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cc6f3-4c07-42b1-ab7e-eec42b9409a0",
   "metadata": {},
   "source": [
    "With a randomly-initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach, a model over time. To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate how much error the model has. The **loss function**, also referred to as the **cost function algorithm** that quantifies how wrong a model is. **Loss**, is the is the measure of this metric. Since loss is the model’s error, we ideally want it to be 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8b2b0-e0ac-44c6-8d91-bfa65a1580fc",
   "metadata": {},
   "source": [
    "Why we do not calculate the error of a model based on the argmax accuracy. take a example of confidence: **[0.22, 0.6, 0.18]** vs **[0.32, 0.36, 0.32]**. If the correct class were indeed the middle one (index 1), the model accuracy would be identical between the two above. But are these two examples really​ as accurate as each other? They are not, because accuracy is simply applying an argmax to the output to find the index of the biggest value. The output of a neural network is actually confidence, and more confidence in the correct answer is better. Because of this, we strive to increase correct confidence and decrease misplaced confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9163a325-84ca-4eaf-ac3c-ff66444bbeb9",
   "metadata": {},
   "source": [
    "***\n",
    "# ***Common Loss Functions***\n",
    "***\n",
    "There are several loss functions used in deep learning, depending on the type of task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91731ac7-43e6-4390-8ed7-5f2e97a114a2",
   "metadata": {},
   "source": [
    "***\n",
    "### ***Categorical Cross-Entropy Loss***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dae7f5-848f-4d90-bd9f-2308eeb8700d",
   "metadata": {},
   "source": [
    "If you are familiar with **linear regression**, then you already know one of the **loss functions** used with neural networks that do regression: **squared error (or mean squared error networks)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2dd044-130c-4346-a5c5-7a607af5fdfb",
   "metadata": {},
   "source": [
    "We are not performing regression in this example; we are classifying, so we need a different loss function. The model has a softmax activation function for the output layer, which means it’s outputting a probability distribution. **Categorical cross-entropy**\n",
    "is explicitly used to compare a **“ground-truth” probability (y ​ or ​ “targets​”**) and some **predicted distribution (y-hat ​ or \n",
    "“predictions​”)**, so it makes sense to use cross-entropy here. It is also one of the most commonly used loss functions with a softmax activation on the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d505b7-d7e0-4de8-92b3-12d3d5280443",
   "metadata": {},
   "source": [
    "The formula for calculating the **categorical cross-entropy** of **y**​ (actual/desired distribution) and **y-hat​** (predicted distribution) is: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88949a5-e6d3-4e18-8f91-75c865334d82",
   "metadata": {},
   "source": [
    "$$\n",
    "L_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **\\(L_i\\)**: Loss for the \\(i^{th}\\) sample.\n",
    "- **\\(j\\)**: Index of the class.\n",
    "- **\\(y_{i,j}\\)**: True label for the \\(i^{th}\\) sample and \\(j^{th}\\) class (one-hot encoded: 1 if true class, 0 otherwise).\n",
    "- **\\(\\hat{y}_{i,j}\\)**: Predicted probability for the \\(i^{th}\\) sample and \\(j^{th}\\) class (output from a softmax function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dca90ea-8fb7-4412-90b4-80b0084ddeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math \n",
    "\n",
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2] \n",
    "\n",
    "# Ground truth \n",
    "target_output = [1, 0, 0] \n",
    "\n",
    "loss = - (math.log(softmax_output[0]) * target_output[0] + \n",
    "          math.log(softmax_output[1]) * target_output[1] + \n",
    "          math.log(softmax_output[2]) * target_output[2] ) \n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d0536-669e-418b-ad64-deced5599fb8",
   "metadata": {},
   "source": [
    "That is the full **categorical cross-entropy calculation**, but we can make a few assumptions given **one-hot target vectors**. First, what are the values for **target_output[1]** and **target_output[2]** in this case? They are both 0, and anything multiplied by 0 is 0. Thus, we do not need to calculate these indices. Next, what is the value for **target_outploss = -math.log(softmax_output[0])ut[0]** in this case? It is 1. So this can be omitted as any number multiplied by 1 remains the same. The same output then, in this example, can be calculated with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e72a30d-82b4-4f70-90f7-6f1119627d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35667494393873245"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss = -math.log(softmax_output[0])\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3503900-96c6-4203-b3ff-3f53871873ca",
   "metadata": {},
   "source": [
    "The **Categorical Cross-Entropy Loss** accounts for that and outputs a larger loss the lower the confidence is: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d505cfef-ded5-46ff-b516-581617958b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-0.05129329438755058\n",
      "-0.10536051565782628\n",
      "-0.2231435513142097\n",
      "...\n",
      "-1.6094379124341003\n",
      "-2.3025850929940455\n",
      "-2.995732273553991\n",
      "-4.605170185988091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(math.log(1.)) \n",
    "print(math.log(0.95)) \n",
    "print(math.log(0.9)) \n",
    "print(math.log(0.8)) \n",
    "print('...') \n",
    "print(math.log(0.2)) \n",
    "print(math.log(0.1)) \n",
    "print(math.log(0.05)) \n",
    "print(math.log(0.01)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9ad7c-9a10-4fd0-a904-2450f56d5f01",
   "metadata": {},
   "source": [
    "We have printed different log values for a few example confidences. When the confidence level equals 1​, meaning the model is 100% **“sure”** about its prediction, the loss value for this sample equals 0​. The loss value raises with the confidence level, approaching 0. You might also wonder why we did not print the result of log(0)​ we’ll explain that shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2473e-3931-42f9-ba68-61c2841632c6",
   "metadata": {},
   "source": [
    "So far, we have applied **log()** to the **softmax output**. For example, 10​^x ​= 100​ can be solved with a log: log​10​ (100)​, which evaluates to 2. This property of the log function is especially ​ beneficial when e ​ (Euler’s number or ~2.71828​) is used in the base (where 10 is in the example). The logarithm with e ​ as its base is referred to as the natural logarithm , natural log , or simply log you may also see this written as ln:ln(x)=log(x)=log​e(x)​ The variety of conventions can make this confusing, so to simplify things, any mention of log will always be a natural logarithm throughout this book. The natural log represents the solution for the x-term in the equation e​^x ​= b​; for example, e​^x ​= 5.2​ is solved by log(5.2)​."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3383f3d-09d3-46aa-b230-e956823dbfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n",
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    " \n",
    "b = 5.2 \n",
    "print(np.log(b)) \n",
    "\n",
    "print(math.e ** np.log(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416dd56f-3fcf-441c-a074-d111161b0dd3",
   "metadata": {},
   "source": [
    "The small difference is the result of floating-point precision. Getting back to the loss calculation, we need to modify our output in two additional ways. First, we wi1ll update our process to work on batches of softmax output distributions; and second, make the negative log calculation dynamic to the target index (the target index has been hard-coded so far). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ea157-e938-4fd6-afd5-1f615b749b91",
   "metadata": {},
   "source": [
    "\n",
    "Consider a scenario with a neural network that performs classification between three classes, and the neural network classifies in batches of three. After running through the softmax activation function with a batch of 3 samples and 3 classes, the network’s output layer yields: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32b4fd73-9904-4103-a184-4a0ce6600854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Probabilities for 3 samples \n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], \n",
    "                            [0.1, 0.5, 0.4], \n",
    "                            [0.02, 0.9, 0.08]]) \n",
    "\n",
    "class_targets = [0, 1, 1]  # dog, cat, cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2ae97-6d2e-402f-97a9-302fb647ca48",
   "metadata": {},
   "source": [
    "We need a way to dynamically calculate the categorical cross-entropy, which we now know is a negative log calculation. To determine which value in the softmax output to calculate the negative log from, we simply need to know our target values. In this example, there are 3 classes; let’s say we are trying to classify something as a **“dog,”** **“cat,”** or **“human.”** A dog is class 0 (at index 0), a cat class 1 (index 1), and a human class 2 (index 2). Let’s assume the batch of three sample inputs to this neural network is being mapped to the target values of a dog, cat, and cat. So the targets (as a list of target indices) would be [0, 1, 1]​.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d9236-32c7-49ec-b2ff-79813a118bf0",
   "metadata": {},
   "source": [
    "The first value, **0**, in class_targets means the first softmax output distribution’s intended prediction was the one at the 0th index of **[0.7, 0.1, 0.2]**; the model has a **0.7** confidence score that this observation is a dog. This continues throughout the batch, where the intended target of the 2nd softmax distribution, **[0.1, 0.5, 0.4]**, was at an index of 1; the model only has a 0.5 confidence score that this is a cat the model is less certain about this observation. In the last sample, it’s also the 2nd index from the softmax distribution, a value of 0.9 in this case a pretty high confidence. \n",
    "    \n",
    "With a collection of softmax outputs and their intended targets, we can map these indices to retrieve the values from the softmax distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b73e741-d765-4aaf-ad95-b37aece4b239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs): \n",
    "    print(distribution[targ_idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c468923-3bca-4808-869a-6debfc16b165",
   "metadata": {},
   "source": [
    " \n",
    "The ***zip()*** function, again, lets us iterate over multiple iterables at the same time in Python. This can be further simplified using NumPy (we are creating a NumPy array of the Softmax outputs this time): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee10d76-e43d-459d-b7f5-242eda268325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], \n",
    "                            [0.1, 0.5, 0.4], \n",
    "                            [0.02, 0.9, 0.08]]) \n",
    "\n",
    "class_targets = [0, 1, 1] \n",
    "\n",
    "print(softmax_outputs[[0, 1, 2], class_targets])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33ad5f-b42c-4988-9003-687a18534324",
   "metadata": {},
   "source": [
    "\n",
    "What are the 0, 1, and 2 values? NumPy lets us index an array in multiple ways. One of them is to use a list filled with indices and that’s convenient for us we could use the class_targets for this purpose as it already contains the list of indices that we are interested in. The problem is that this has to filter data rows in the array the second dimension. To perform that, we also need to \n",
    "explicitly filter this array in its first dimension. This dimension contains the predictions and we, of course, want to retain them all. We can achieve that by using a list containing numbers from 0 through all of the indices. We know we are going to have as many indices as distributions in our entire batch, so we can use a **range()** instead of typing each value ourselves:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd20a8ba-ab1a-4c6f-9052-84607c93299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(softmax_outputs[ \n",
    "    range(len(softmax_outputs)), class_targets \n",
    "]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddaa31b-2031-4059-a217-3d17cea536b5",
   "metadata": {},
   "source": [
    "This returns a list of the confidences at the target indices for each of the samples. Now we apply the negative log to this list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07c0c9c6-ad23-456c-8490-c6ac2e3f6c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(-np.log(softmax_outputs[ \n",
    "    range(len(softmax_outputs)), class_targets \n",
    "])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34c269-d9bf-4edc-83c2-84c3cedc1b80",
   "metadata": {},
   "source": [
    "Finally, we want an average loss per batch to have an idea about how our model is doing during training. There are many ways to calculate an average in Python; the most basic form of an average is the **arithmetic mean** **: sum(iterable) / len(iterable)​**. NumPy has a method that computes this average on arrays, so we will use that instead. We add NumPy’s average to the code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "124ba395-bdee-4003-a6f7-aeb057a08dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "\n",
    "neg_log = -np.log(softmax_outputs[ \n",
    "              range(len(softmax_outputs)), class_targets \n",
    "          ]) \n",
    "\n",
    "average_loss = np.mean(neg_log) \n",
    "\n",
    "print(average_loss) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc1365-b846-4e14-a9f3-271288259ddb",
   "metadata": {},
   "source": [
    "We have already learned that targets can be one-hot encoded, where all values, except for one, are zeros, and the correct label’s position is filled with 1. They can also be sparse, which means that the numbers they contain are the correct class numbers we are generating them this way with the **spiral_data()** function, and we can allow the loss calculation to accept any of these forms. Since we implemented this to work with sparse labels (as in our training data), we have to add a check if they are one-hot encoded and handle it a bit differently in this new case. The check can be performed by counting the dimensions if targets are single-dimensional (like a list), they are sparse, but if there are 2 dimensions (like a list of lists), then there is a set of one-hot encoded vectors. In this second case, we will implement a solution using the first equation from this chapter, instead of filtering out the confidences at the target labels. We have to multiply confidences by the targets, zeroing out all values except the ones at correct labels, performing a sum along the row axis (axis 1​). We have to add a test to the code we just wrote for the number of dimensions, move calculations of the log values outside of this new if​ statement, and implement the solution for the one-hot encoded labels following the first equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99ed0e67-956a-494d-be62-267e2891bf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np \n",
    " \n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], \n",
    "                            [0.1, 0.5, 0.4], \n",
    "                            [0.02, 0.9, 0.08]]) \n",
    "\n",
    "class_targets = np.array([[1, 0, 0], \n",
    "                          [0, 1, 0], \n",
    "                          [0, 1, 0]]) \n",
    "\n",
    "# Probabilities for target values - only if categorical labels \n",
    "if len(class_targets.shape) == 1: \n",
    "    correct_confidences = softmax_outputs[ \n",
    "        range(len(softmax_outputs)), \n",
    "        class_targets \n",
    "    ] \n",
    "    \n",
    "# Mask values - only for one-hot encoded labels \n",
    "elif len(class_targets.shape) == 2: \n",
    "    correct_confidences = np.sum( \n",
    "    softmax_outputs * class_targets, \n",
    "    axis=1\n",
    "    ) \n",
    "\n",
    "# Losses \n",
    "neg_log = -np.log(correct_confidences) \n",
    "average_loss = np.mean(neg_log) \n",
    "print(average_loss) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ffd62-9543-4c6f-95f4-e95dca4da7e1",
   "metadata": {},
   "source": [
    "***\n",
    "### ***The Categorical Cross-Entropy Loss Class***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9632ffa8-bbc6-40fa-b250-fde980bf3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Common loss class \n",
    "class Loss:\n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y):\n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f84e03-e1ec-406f-882f-e40bb0b183d4",
   "metadata": {},
   "source": [
    "Let’s convert our loss code into a class for convenience down the line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44e9f0d1-9662-4882-8f92-963eae9c4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cross-Entropy Loss Class\n",
    "class LossCategoricalCrossentropy(Loss):\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip predictions to prevent log(0)\n",
    "        # Clip both sides to avoid shifting the mean\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Handle categorical labels (integer labels)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        # Handle one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        \n",
    "        # Compute the negative log likelihoods\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        \n",
    "        return negative_log_likelihoods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6c64a-a524-4f73-80b0-16e2e9d97a19",
   "metadata": {},
   "source": [
    "This class inherits the Loss class and performs all the error calculations that we derived throughout this chapter and can be used as an object. For example, using the manually-created output and targets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c3eab18-d440-454c-a25b-da971fbb81a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_function = LossCategoricalCrossentropy() \n",
    "loss = loss_function.calculate(softmax_outputs, class_targets) \n",
    "print(loss) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecc004-c41f-43f9-8580-ff7eec0c3c09",
   "metadata": {},
   "source": [
    "*** \n",
    "### ***Accuracy Calculation***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471fad6-a408-4cd6-bad2-9801180fa36a",
   "metadata": {},
   "source": [
    "While loss is a useful metric for optimizing a model, the metric commonly used in practice along with loss is the **accuracy**, which describes how often the largest confidence is the correct class in terms of a fraction. Conveniently, we can reuse existing variable definitions to calculate the accuracy metric. We will use the argmax ​values from the softmax outputs ​and then compare these to the targets. This is as simple as doing (note that we slightly modified the softmax_outputs for the purpose of this example): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41856cad-95f8-40a8-9a8f-59a6d36bdaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Probabilities of 3 samples (output of softmax)\n",
    "softmax_outputs = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.5, 0.1, 0.4],\n",
    "    [0.02, 0.9, 0.08]\n",
    "])\n",
    "\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# Get predicted class indices from softmax outputs\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "\n",
    "# If targets are one-hot encoded, convert them to class indices\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "\n",
    "# Calculate accuracy (True -> 1, False -> 0)\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eae896-cb91-44c9-b673-006c5ff87ed4",
   "metadata": {},
   "source": [
    "We are also handling one-hot encoded targets by converting them to sparse values using np.argmax(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f8ed6be-61ce-4c85-afa2-8d6d3503dcde",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+200B (2366397136.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[45], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    predictions = np.argmax(activation2.output, axis​=1)\u001b[0m\n\u001b[1;37m                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+200B\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(activation2.output, axis​=1) \n",
    "if len(y.shape) == 2: \n",
    "   y \n",
    "= np.argmax(y, axis​=1) \n",
    "accuracy = np.mean(predictions==y) \n",
    "# Print accuracy \n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76794a-6a58-45d3-a478-4cc10fc78658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
