{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ef4cf0-2e58-4f29-84bd-7763033e69ca",
   "metadata": {},
   "source": [
    "***\n",
    "# <center>***Validation Data***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340ef6f-d222-4c35-8ceb-6751c2d7b9a6",
   "metadata": {},
   "source": [
    "In **optimization**, we used **hyperparameter tuning** to select hyperparameters that lead to better results, but one more thing requires clarification. We should not​ check different hyperparameters using the test dataset, if we do that, we are going to be manually optimizing the model to the test dataset, biasing it towards overfitting these data, and these data are supposed to be used only to perform the last check if the model trains and generalizes well. In other words, if we are tuning our network’s parameters to fit the testing data, then we are essentially optimizing our network on the testing data, which is another way for overfitting on these data. \n",
    "\n",
    "Thus, **hyperparameter tuning** using the test dataset is a mistake. The test dataset should only be used as unseen data, not informing the model in any way, which hyperparameter tuning is, other than to test performance. \n",
    "\n",
    "Hyperparameter tuning can be performed using yet another dataset called **validation data**. The test dataset needs to contain real out-of-sample data, but with a validation dataset, we have more freedom with choosing data. If we have a lot of training data and can afford to use some for validation purposes, we can take it as an out-of-sample dataset, similar to a test dataset. We can now search for parameters that work best using this new validation dataset and test our model at the end using the test dataset to see if we really tuned the model or just overfitted it to the validation data. \n",
    "\n",
    "There are situations when we’ll be short on data and cannot afford to create yet another dataset from the training data. In those situations, we have two options:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a6dec9-7348-416d-8e78-1b76a2380ebb",
   "metadata": {},
   "source": [
    "The first is to temporarily split the training data into a smaller training dataset and validation dataset for hyperparameter tuning. Afterward, with the final hyperparameter set, train the model on all the training data. We allow ourselves to do that as we tune the model to the part of training data that we put aside as validation data. Keep in mind that we still have a test dataset to check the \n",
    "model’s performance after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ab7ee-14c9-49ff-8a44-012782919f5e",
   "metadata": {},
   "source": [
    "The second possibility in situations where we are short on data is a process called **cross-validation**. Cross-validation is primarily used when we have a small training dataset and cannot afford any data for validation purposes. How it works is we split the training dataset into a given number of parts, let’s say 5. We now train the model on the first 4 chunks and validate it on the last. So far, this is similar to the case described previously we are also only using the training dataset and can validate on data that was not used for training. What makes cross-validation different is that we then swap samples. For example, if we have 5 chunks, we can call them chunks A, B, C, D, and E. We may first train on A, B, C, and D, then validate on E. We will then train on A, B, C, E, and validate on D, doing this until we have validated on each of the 5 sample groups. This way, we do not lose any training data. We validate using the data that was not used for training during any given iteration and validate on more data than if we just temporarily split the training dataset and train on all of the samples. This validation method is often called **k-fold cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d677253-bc35-4a71-8016-cd20cd25c376",
   "metadata": {},
   "source": [
    "When using a **validation dataset** and **cross-validation**, it is common to loop over different hyperparameter sets, leaving the code to run training multiple times, applying different settings each run, and reviewing the results to choose the best set of hyperparameters. In general, we should not loop over all​ possible setting combinations that we would like to check unless training \n",
    "is exceptionally fast. It’s usually better to check some settings that we suspect will work well, pick the best combination of those settings, tweak them to create the next list of setting sets, and train the model on new sets. We can repeat this process as many times as we’d like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663385d-01dd-4a42-9464-e6c59c80cbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
