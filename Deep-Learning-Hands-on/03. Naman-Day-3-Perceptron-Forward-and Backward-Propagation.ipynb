{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c20916-dffe-46ff-bf05-364b69834477",
   "metadata": {},
   "source": [
    "# Day 3: What is a Perceptron? Forward and Backward Propagation\n",
    "\n",
    "Day 3 demonstrates the implementation of a single perceptron for solving a binary classification problem (AND gate) using forward propagation and backward propagation in TensorFlow. The code is implemented without Keras to understand the foundational concepts of neural networks.\n",
    "\n",
    "## What I Learned\n",
    "\n",
    "### 1. **What is a Perceptron?**\n",
    "- A perceptron is a type of artificial neuron used in machine learning. It is the simplest type of neural network and is used for binary classification. The perceptron performs the following operations:\n",
    "    - #### Weighted Sum:\n",
    "        - z = W . X + b\n",
    "    - Where:\n",
    "        - \\( W \\): Weights\n",
    "        - \\( X \\): Input features\n",
    "        - \\( b \\): Bias term\n",
    "\n",
    "### 2. **Activation Function?**\n",
    "- An **activation function** applies a transformation to the weighted sum of inputs, determining whether a neuron should be activated or not. It introduces **non-linearity** to the model, enabling the neural network to learn complex patterns.\n",
    "\n",
    "    #### **Common Activation Functions:**\n",
    "    1. **Step Function**:  \n",
    "       A binary function that outputs either 0 or 1 based on whether the weighted sum exceeds a threshold.\n",
    "    \n",
    "       \\[\n",
    "       f(z) =\n",
    "       \\begin{cases}\n",
    "       1 & \\text{if } z \\geq 0 \\\\\n",
    "       0 & \\text{if } z < 0\n",
    "       \\end{cases}\n",
    "       \\]\n",
    "    \n",
    "    2. **Sigmoid Function**:  \n",
    "       Maps the input to a range between 0 and 1. It is commonly used in binary classification.\n",
    "    \n",
    "       \\[\n",
    "       f(z) = \\frac{1}{1 + e^{-z}}\n",
    "       \\]\n",
    "    \n",
    "    3. **ReLU (Rectified Linear Unit)**:  \n",
    "       Outputs the input directly if it is positive, and 0 if it is negative. It is commonly used in hidden layers of deep networks.\n",
    "    \n",
    "       \\[\n",
    "       f(z) = \\max(0, z)\n",
    "       \\]\n",
    "    \n",
    "    4. **Tanh (Hyperbolic Tangent)**:  \n",
    "       Maps the input to a range between -1 and 1, similar to the sigmoid but centered around 0.\n",
    "    \n",
    "       \\[\n",
    "       f(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "       \\]\n",
    "\n",
    "    #### **Purpose:**\n",
    "    - Introduces non-linearity, allowing the neural network to learn complex patterns.\n",
    "    - Helps the model make decisions based on the weighted input sum.\n",
    "\n",
    "### 3. **Forward Propagation**\n",
    "\n",
    "- **Forward propagation** is the process by which input data is passed through the layers of a neural network to generate an output. It involves calculating the weighted sum of inputs, adding the bias, and applying the activation function to produce the final prediction.\n",
    "\n",
    "    ### Steps in Forward Propagation:\n",
    "     1. **Input Layer**:  \n",
    "        - The input data \\( X \\) is passed into the neural network. This could be an image, text, or any other type of data.\n",
    "\n",
    "     2. **Weighted Sum**:  \n",
    "        - The inputs are multiplied by the corresponding weights \\( W \\) and added to the bias term \\( b \\).\n",
    "     \n",
    "        \\[\n",
    "        z = W \\cdot X + b\n",
    "        \\]\n",
    "  \n",
    "     3. **Activation Function**:  \n",
    "        - The weighted sum \\( z \\) is passed through an activation function \\( f(z) \\) (such as sigmoid, ReLU, or tanh) to introduce non-linearity. This determines whether the neuron is activated.\n",
    "\n",
    "             \\[\n",
    "             a = f(z)\n",
    "             \\]\n",
    "\n",
    "     4. **Output**:  \n",
    "        - The output \\( a \\) is passed to the next layer or, in the case of the final layer, returned as the model's prediction.\n",
    "\n",
    "    ### Forward Propagation in a Single Layer:\n",
    "    - In a simple neural network with one layer:\n",
    "        - Each input feature \\( X \\) is multiplied by a weight \\( W \\).\n",
    "        - The bias term \\( b \\) is added to the weighted sum.\n",
    "        - The result is passed through an activation function to obtain the output.\n",
    "\n",
    "This process is repeated layer by layer, from the input layer to the output layer, until a final prediction is made.\n",
    "\n",
    "\n",
    "## 4. **Backward Propagation**\n",
    "- **Backward propagation**, or **backpropagation**, is the process used to train a neural network by adjusting its weights and biases based on the error in its predictions. It calculates the gradients of the loss function with respect to each parameter using the chain rule and updates the parameters to minimize the error.\n",
    "\n",
    "    ### Steps in Backward Propagation:\n",
    "    \n",
    "     1. **Compute the Loss**:  \n",
    "        - The loss function \\( L \\) quantifies the difference between the predicted output and the actual target. Common loss functions include:\n",
    "          - Mean Squared Error (MSE) for regression tasks.\n",
    "          - Cross-Entropy Loss for classification tasks.\n",
    "  \n",
    "     2. **Calculate Gradients**:  \n",
    "        - The gradients of the loss function with respect to the weights (\\( W \\)) and biases (\\( b \\)) are computed using the **chain rule** of calculus.\n",
    "\n",
    "    \n",
    "     3. **Weight and Bias Updates**:  \n",
    "        - The weights and biases are updated using the gradients and a learning rate \\( \\eta \\) (step size) to minimize the loss.\n",
    "     \n",
    "     4. **Iterate Through Layers**:  \n",
    "        - Backpropagation starts at the output layer and propagates the error backward through each hidden layer, updating weights and biases at every step.\n",
    "     \n",
    "    ### Purpose of Backward Propagation:\n",
    "     - **Minimize the Loss**: Helps the network learn by reducing the difference between predicted and actual outputs.\n",
    "     - **Efficient Training**: Ensures that weights and biases are updated systematically to improve the model's performance.\n",
    "\n",
    "Backward propagation is an essential step in training neural networks and enables them to learn from data effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e802385-f046-4e50-a33c-8f56242883de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3dcc94-5c7a-4412-bd64-4fe0c24c2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data: OR Gate (Linearly separable)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [1]], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b53d13-6996-4608-b726-e0e35c0d2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters: Weights and Bias\n",
    "weights = tf.Variable(tf.random.normal([2, 1]))\n",
    "bias = tf.Variable(tf.random.normal([1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fcf2101-a8d3-4852-8c76-45cacf9a31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6d69f4-2585-4270-ae87-5d97a2c7b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + tf.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30fdf19f-79d4-43a8-9494-252c236fd473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss function: Mean Squared Error\n",
    "def compute_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c973a6ea-f874-47fd-a1c4-e5a1bb58ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training the perceptron\n",
    "epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13bc7300-799e-454f-be47-83ac542bf512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.637333869934082\n",
      "Epoch 100, Loss: 0.16182009875774384\n",
      "Epoch 200, Loss: 0.0965847447514534\n",
      "Epoch 300, Loss: 0.08095353096723557\n",
      "Epoch 400, Loss: 0.06975092738866806\n",
      "Epoch 500, Loss: 0.06086064502596855\n",
      "Epoch 600, Loss: 0.05366594344377518\n",
      "Epoch 700, Loss: 0.04776450991630554\n",
      "Epoch 800, Loss: 0.04286418482661247\n",
      "Epoch 900, Loss: 0.038749516010284424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward propagation\n",
    "        linear_output = tf.matmul(X, weights) + bias \n",
    "        predictions = sigmoid(linear_output) \n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(y, predictions)\n",
    "    \n",
    "    # Backward propagation\n",
    "    gradients = tape.gradient(loss, [weights, bias])\n",
    "    weights.assign_sub(learning_rate * gradients[0]) \n",
    "    bias.assign_sub(learning_rate * gradients[1])\n",
    "\n",
    "    # Logging progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f32cc287-41d2-4b05-90ad-33fae64e8af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[2.5266373]\n",
      " [2.465813 ]], Bias: [-0.8859231]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final weights and bias\n",
    "print(f\"Weights: {weights.numpy()}, Bias: {bias.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd9a5fff-4fa5-41c7-bb2e-639b33aab543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.8291889 ]\n",
      " [0.98380184]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing\n",
    "test_data = np.array([[0, 1], [1, 1]], dtype=np.float32)\n",
    "test_predictions = sigmoid(tf.matmul(test_data, weights) + bias)\n",
    "print(f\"Predictions: {test_predictions.numpy()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
